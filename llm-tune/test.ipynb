{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import partial\n",
    "from accelerate import Accelerator\n",
    "from accelerate.checkpointing import save_accelerator_state\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import sys \n",
    "# sys.path.append(\"/home/LLM_compression/transformers_modified/src\")\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    LoraConfig\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from quant_utils import get_fp_llama, make_layer_bits, prepare_llama_quant\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    token: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n",
    "                \"generated when running `huggingface-cli login` (stored in `~/.huggingface`).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\"\n",
    "                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n",
    "                \"execute code present on the Hub on your local machine.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    max_memory: int = field(\n",
    "        default=21,\n",
    "        metadata={\"help\": \"Free memory per gpu.\"}\n",
    "    )\n",
    "    lora_init: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"True: Use zero and gaussian initialization; False: Load adapters from LoftQ in HF hub.\"},\n",
    "    )\n",
    "    rank: int = field(\n",
    "        default=64,\n",
    "        metadata={\"help\": \"Rank of LoRA adapters. LoftQ does not require this config. Used for fp16 LoRA or QLoRA.\"},\n",
    "    )\n",
    "    lora_alpha: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"LoftQ does not require this config. Used for QLoRA.\"},\n",
    "    )\n",
    "    quant_noise_config: dict = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Parameters to add noise\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether or not to allow for custom dataset defined on the Hub in their own modeling files. This option\"\n",
    "                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n",
    "                \"execute code present on the Hub on your local machine.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Optional input sequence length after tokenization. \"\n",
    "                \"The training dataset will be truncated in block of this size for training. \"\n",
    "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    dataset_percentage: Optional[int] = field(\n",
    "        default=100,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the dataset used for computation\"\n",
    "        },  \n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def encode_with_prompt_completion_format(example, tokenizer, max_seq_length):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "    '''\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "        example_text = example['prompt'] + ' ' + example['completion']\n",
    "    else:\n",
    "        example_text = example['prompt'] + example['completion']\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "    tokenized_prompt = tokenizer(example['prompt'], return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    # mask the prompt part for avoiding loss\n",
    "    labels[:, :tokenized_prompt.input_ids.shape[1]] = -100\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                message_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    _concat_messages(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = _concat_messages(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt', \n",
    "                max_length=max_seq_length, \n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            labels[:, message_start_idx:message_end_idx] = -100\n",
    "            \n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "def load_hf_datasets(\n",
    "    data_args\n",
    "):\n",
    "    # Load the dataset\n",
    "    if data_args.dataset_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        raw_datasets = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            streaming=data_args.streaming,\n",
    "            trust_remote_code=data_args.trust_remote_code\n",
    "        )\n",
    "\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets[\"validation\"] = load_dataset(\n",
    "                data_args.dataset_name,\n",
    "                data_args.dataset_config_name,\n",
    "                split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "                streaming=data_args.streaming,\n",
    "                trust_remote_code=data_args.trust_remote_code\n",
    "            )\n",
    "            raw_datasets[\"train\"] = load_dataset(\n",
    "                data_args.dataset_name,\n",
    "                data_args.dataset_config_name,\n",
    "                split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "                streaming=data_args.streaming,\n",
    "                trust_remote_code=data_args.trust_remote_code\n",
    "            )\n",
    "        \n",
    "        if data_args.dataset_percentage < 100:\n",
    "            dataset_frac = data_args.dataset_percentage/100\n",
    "            dataset_parts = raw_datasets['train'].train_test_split(train_size=dataset_frac)\n",
    "            raw_datasets['train'] = dataset_parts['train']\n",
    "            dataset_parts = raw_datasets['validation'].train_test_split(test_size=dataset_frac)\n",
    "            raw_datasets['validation'] = dataset_parts['test']\n",
    "\n",
    "        return raw_datasets\n",
    "\n",
    "def read_config(conf_path, func_name: str):\n",
    "    if isinstance(conf_path, str):\n",
    "        conf_path = Path(conf_path)\n",
    "\n",
    "    source = conf_path.read_text()\n",
    "    bytecode = compile(source, conf_path.as_posix(), \"exec\")\n",
    "    namespace = {\n",
    "        \"__file__\": conf_path.as_posix(),\n",
    "    }\n",
    "    exec(bytecode, namespace)\n",
    "    return namespace[func_name]()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory info: \n",
      "(5929172992, 25388515328)\n",
      "Restricting the memory to 0.7612636100341251 of the total memory to have a limit of 18 (0.7612636100341251 x 23.6448974609375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": [
    "config_path = '/home/LLM_compression/llm-tune/configs/llama_instruct_megatron.py'\n",
    "config = read_config(config_path, 'model_configs')\n",
    "\n",
    "config_dict = dict(config)\n",
    "config_dict['data'] = dict(config_dict['data'])\n",
    "config_dict['quant_noise_config'] = dict(config_dict['quant_noise_config'])\n",
    "config_dict['outliers'] = dict(config_dict['outliers'])\n",
    "config_dict['QuantizedLinear'] = dict(config_dict['QuantizedLinear'])\n",
    "config_dict['NoiseQuant'] = dict(config_dict['NoiseQuant'])\n",
    "config_dict['BitNoiseQuant'] = dict(config_dict['BitNoiseQuant'])\n",
    "config = config_dict\n",
    "\n",
    "data_args = DataTrainingArguments(\n",
    "    dataset_name = config['data']['dataset_name'],\n",
    "    dataset_config_name = config['data']['dataset_config_name'],\n",
    "    validation_split_percentage = config['data']['validation_split_percentage'],\n",
    "    max_seq_length = config['data']['max_seq_length'],\n",
    "    dataset_percentage = config['data']['dataset_percentage'],\n",
    "    trust_remote_code = config['data']['trust_remote_code'],\n",
    "    preprocessing_num_workers = config['data']['preprocessing_num_workers']\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path = config['model_name_or_path'], #\"/home/projects/LLaMA/huggingface/Llama-2-7b-hf\",\n",
    "    config_name = config['model_config_name'], #\"/home/projects/LLaMA/huggingface/Llama-2-7b-hf/config.json\",\n",
    "    tokenizer_name = config['tokenizer_name'], #\"/home/projects/LLaMA/huggingface/Llama-2-7b-hf\",\n",
    "    use_fast_tokenizer = config['use_fast_tokenizer'],\n",
    "    token = config['token'], #None,\n",
    "    trust_remote_code = config['trust_remote_code'],\n",
    "    max_memory = config['max_memory'],\n",
    "    # cache_dir= config.cache_dir,\n",
    "    rank = config['lora_rank'],\n",
    "    lora_alpha = config['lora_alpha'],\n",
    "    # quant_noise_config = config['quant_noise_config']\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # run_name=config.run_name,\n",
    "    output_dir = config['output_dir'],\n",
    "    overwrite_output_dir = True,\n",
    "    learning_rate = config['learning_rate'], \n",
    "    seed = config['seed'], \n",
    "    max_steps = config['max_steps'],\n",
    "    # num_train_epochs = config.num_train_epochs, #3,\n",
    "    weight_decay = config['weight_decay'], #0.1,\n",
    "    warmup_ratio = config['warmup_ratio'],\n",
    "    lr_scheduler_type = config['lr_scheduler_type'],\n",
    "    per_device_train_batch_size = config['per_device_train_batch_size'], #2,\n",
    "    per_device_eval_batch_size = config['per_device_eval_batch_size'], #2,\n",
    "    gradient_accumulation_steps = config['gradient_accumulation_steps'], #16,\n",
    "    gradient_checkpointing=config['gradient_checkpointing'], #False,\n",
    "    save_strategy = config['save_strategy'],\n",
    "    save_steps = config['save_steps'],\n",
    "    # evaluation_strategy = config.evaluation_strategy,\n",
    "    # eval_steps = config.eval_steps,\n",
    "    logging_steps = 1,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    # report_to = config['report_to']\n",
    ")\n",
    "\n",
    "# If limit on cuda memory is specified enforce the limit\n",
    "if model_args.max_memory > 0:\n",
    "    mem_info = torch.cuda.mem_get_info()\n",
    "    print(\"Memory info: \\n{}\".format(mem_info))\n",
    "    # total_memory = mem_info[1] * 1e-9 # convert Bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory  * 2**(-30) # convert Bytes to GB\n",
    "    if model_args.max_memory > total_memory:\n",
    "        raise ValueError(\"The specified memory limit {} is greater than the available memory {}.\".format(model_args.max_memory, total_memory))\n",
    "    else:\n",
    "        fraction = model_args.max_memory / total_memory\n",
    "        torch.cuda.set_per_process_memory_fraction(fraction)\n",
    "        print(\"Restricting the memory to {} of the total memory to have a limit of {} ({} x {})\".format(fraction, model_args.max_memory, fraction, total_memory))\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"token\": model_args.token,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "\n",
    "# Load pretrained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=model_args.token,\n",
    "    # device_map = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_trained = model.model.layers[0].self_attn.q_proj.quantizer.alpha_scale.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum-cli export onnx --task text-generation --model /home/Quantization/weights_study/weights/llama7b_3bit_loaded ./onnx_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): QuantizedLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (quantizer): SymQuant()\n",
       "          )\n",
       "          (k_proj): QuantizedLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (quantizer): SymQuant()\n",
       "          )\n",
       "          (v_proj): QuantizedLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (quantizer): SymQuant()\n",
       "          )\n",
       "          (o_proj): QuantizedLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (quantizer): SymQuant()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantizedLinear(\n",
       "            in_features=4096, out_features=11008, bias=False\n",
       "            (quantizer): SymQuant()\n",
       "          )\n",
       "          (up_proj): QuantizedLinear(\n",
       "            in_features=4096, out_features=11008, bias=False\n",
       "            (quantizer): SymQuant()\n",
       "          )\n",
       "          (down_proj): QuantizedLinear(\n",
       "            in_features=11008, out_features=4096, bias=False\n",
       "            (quantizer): SymQuant()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimum.onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'optimum.onnxruntime' from '/opt/conda/lib/python3.10/site-packages/optimum/onnxruntime/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum.onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0349, -0.0058,  0.0396,  ..., -0.0161,  0.1279, -0.0388],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.data[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0349, -0.0059,  0.0396,  ..., -0.0160,  0.1279, -0.0388],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.data[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0349, -0.0059,  0.0396,  ..., -0.0160,  0.1279, -0.0388],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.data[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -2., -0.,  ...,  1.,  0., -1.],\n",
       "        [ 2., -0.,  0.,  ..., -1., -1.,  1.],\n",
       "        [-1.,  1.,  0.,  ...,  1.,  2., -0.],\n",
       "        ...,\n",
       "        [-0.,  1., -0.,  ...,  1., -2.,  1.],\n",
       "        [ 2.,  1.,  0.,  ..., -2., -1., -1.],\n",
       "        [-1., -1.,  0.,  ...,  2.,  2., -1.]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0062, -0.0124, -0.0000,  ...,  0.0062,  0.0000, -0.0062],\n",
       "        [ 0.0209, -0.0000,  0.0000,  ..., -0.0104, -0.0104,  0.0104],\n",
       "        [-0.0104,  0.0104,  0.0000,  ...,  0.0104,  0.0208, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0129, -0.0000,  ...,  0.0129, -0.0258,  0.0129],\n",
       "        [ 0.0300,  0.0150,  0.0000,  ..., -0.0300, -0.0150, -0.0150],\n",
       "        [-0.0092, -0.0092,  0.0000,  ...,  0.0183,  0.0183, -0.0092]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.model.layers[0].self_attn.q_proj.weight.data\n",
    "alpha = model.model.layers[0].self_attn.q_proj.quantizer.alpha_scale\n",
    "bit = model.model.layers[0].self_attn.q_proj.quantizer.bit\n",
    "mask = model.model.layers[0].self_attn.q_proj.quantizer.mask\n",
    "inv_col_perm = model.model.layers[0].self_attn.q_proj.inv_col_perm\n",
    "qmax = 2**(bit-1) - 1\n",
    "scale = alpha / qmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.is_quant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_weight = w[:, mask]\n",
    "fp_weight = w[:, ~mask].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dq = model.model.layers[0].self_attn.q_proj.quantizer(int_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 3968])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_dq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_out = torch.hstack([w_dq, fp_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_out = w_out[:, inv_col_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0349, -0.0059,  0.0396,  ..., -0.0160,  0.1279, -0.0388],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_out[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -2., -0.,  ...,  1.,  0., -1.],\n",
       "        [ 2., -0.,  0.,  ..., -1., -1.,  1.],\n",
       "        [-1.,  1.,  0.,  ...,  1.,  2., -0.],\n",
       "        ...,\n",
       "        [-0.,  1., -0.,  ...,  1., -2.,  1.],\n",
       "        [ 2.,  1.,  0.,  ..., -2., -1., -1.],\n",
       "        [-1., -1.,  0.,  ...,  2.,  2., -1.]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0349, -0.0059,  0.0396,  ..., -0.0160,  0.1279, -0.0388],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0124,  0.0267,  0.0204,  ...,  0.0062,  0.0303,  0.0591],\n",
       "        [-0.1455, -0.0396, -0.0713,  ..., -0.0376,  0.0079,  0.0317],\n",
       "        [ 0.2500,  0.0452,  0.0439,  ...,  0.1216, -0.0444, -0.0610],\n",
       "        ...,\n",
       "        [-0.0344, -0.0242, -0.0698,  ...,  0.0124, -0.0294,  0.0439],\n",
       "        [-0.0028,  0.0023, -0.0496,  ...,  0.0164, -0.0146, -0.0601],\n",
       "        [-0.0106,  0.0120, -0.0114,  ...,  0.0282,  0.0366,  0.0120]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:, ~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0098, -0.0417,  0.0391,  ...,  0.0025, -0.0674,  0.0315],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0098, -0.0417,  0.0391,  ...,  0.0025, -0.0674,  0.0315],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0062, -0.0124, -0.0000,  ...,  0.0062,  0.0000, -0.0062],\n",
       "        [ 0.0209, -0.0000,  0.0000,  ..., -0.0104, -0.0104,  0.0104],\n",
       "        [-0.0103,  0.0103,  0.0000,  ...,  0.0103,  0.0206, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0129, -0.0000,  ...,  0.0129, -0.0259,  0.0129],\n",
       "        [ 0.0299,  0.0150,  0.0000,  ..., -0.0299, -0.0150, -0.0150],\n",
       "        [-0.0092, -0.0092,  0.0000,  ...,  0.0183,  0.0183, -0.0092]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha / qmax * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0098, -0.0417,  0.0391,  ...,  0.0025, -0.0674,  0.0315],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_indices = quant_params['model.layers.0.self_attn.q_proj']['fp_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3190,  588, 2518, 2993, 1808, 2750, 4015, 2077, 2522, 2820, 2469, 2641,\n",
       "        3601,  760, 2958, 3275, 1375, 1218, 3213, 1391, 2196,  447, 2637, 2350,\n",
       "        1346,  575, 2580, 2314, 1791, 1553, 2050, 2092,  363,  642,  326, 3656,\n",
       "        1571, 3306, 1159, 2158, 3842, 2235,  462, 3492, 3946, 1404, 3863,   94,\n",
       "         613, 1261, 2298, 3135,  310, 2608,  289, 2789, 2403, 2028, 2147, 4076,\n",
       "        1076,  490, 1512, 2593, 3431, 4031, 3209,  339, 2232, 3953,  959,  210,\n",
       "         391, 2744,  125, 3222, 2944, 1456, 2866, 1544,  580, 2914, 3893, 2533,\n",
       "        3797,  597, 2393, 1710, 3045, 2778, 1626, 2136, 3729, 2853, 3877, 3550,\n",
       "        2317, 3933, 1813, 3920,  257,  102, 1996,  288,  577, 1788,  473, 2704,\n",
       "        3915, 1995, 1415, 3238, 3078, 2130, 4071, 2927, 1622, 3164, 1411, 1110,\n",
       "        3178, 2543,   22, 3443, 4030, 3964, 1744, 4051])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_indices[122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  22,   94,  102,  125,  210,  257,  288,  289,  310,  326,  339,  363,\n",
       "         391,  447,  462,  473,  490,  575,  577,  580,  588,  597,  613,  642,\n",
       "         760,  959, 1076, 1110, 1159, 1218, 1261, 1346, 1375, 1391, 1404, 1411,\n",
       "        1415, 1456, 1512, 1544, 1553, 1571, 1622, 1626, 1710, 1744, 1788, 1791,\n",
       "        1808, 1813, 1995, 1996, 2028, 2050, 2077, 2092, 2130, 2136, 2147, 2158,\n",
       "        2196, 2232, 2235, 2298, 2314, 2317, 2350, 2393, 2403, 2469, 2518, 2522,\n",
       "        2533, 2543, 2580, 2593, 2608, 2637, 2641, 2704, 2744, 2750, 2778, 2789,\n",
       "        2820, 2853, 2866, 2914, 2927, 2944, 2958, 2993, 3045, 3078, 3135, 3164,\n",
       "        3178, 3190, 3209, 3213, 3222, 3238, 3275, 3306, 3431, 3443, 3492, 3550,\n",
       "        3601, 3656, 3729, 3797, 3842, 3863, 3877, 3893, 3915, 3920, 3933, 3946,\n",
       "        3953, 3964, 4015, 4030, 4031, 4051, 4071, 4076])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_indices.sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([122,  47, 101,  74,  71, 100, 103,  54,  52,  34,  67,  32,  72,  21,\n",
       "         42, 106,  61,  25, 104,  80,   1,  85,  48,  33,  13,  70,  60, 119,\n",
       "         38,  17,  49,  24,  16,  19,  45, 118, 110,  77,  62,  79,  29,  36,\n",
       "        116,  90,  87, 126, 105,  28,   4,  98, 109, 102,  57,  30,   7,  31,\n",
       "        113,  91,  58,  39,  20,  68,  41,  50,  27,  96,  23,  86,  56,  10,\n",
       "          2,   8,  83, 121,  26,  63,  53,  22,  11, 107,  73,   5,  89,  55,\n",
       "          9,  93,  78,  81, 115,  76,  14,   3,  88, 112,  51, 117, 120,   0,\n",
       "         66,  18,  75, 111,  15,  37,  64, 123,  43,  95,  12,  35,  92,  84,\n",
       "         40,  46,  94,  82, 108,  99,  97,  44,  69, 125,   6, 124,  65, 127,\n",
       "        114,  59])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_indices.sort()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0124,  0.0267,  0.0204,  ...,  0.0062,  0.0303,  0.0591],\n",
       "        [-0.1458, -0.0396, -0.0711,  ..., -0.0377,  0.0079,  0.0318],\n",
       "        [ 0.2510,  0.0451,  0.0439,  ...,  0.1216, -0.0443, -0.0609],\n",
       "        ...,\n",
       "        [-0.0345, -0.0242, -0.0696,  ...,  0.0124, -0.0294,  0.0439],\n",
       "        [-0.0028,  0.0024, -0.0497,  ...,  0.0164, -0.0146, -0.0600],\n",
       "        [-0.0106,  0.0120, -0.0113,  ...,  0.0282,  0.0365,  0.0120]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_params['model.layers.0.self_attn.q_proj']['fp_weight'][:, fp_indices.sort()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['QuantizedLinear']['replace']:\n",
    "    outliers_config= config['outliers']\n",
    "    outlier_ids = get_fp_llama(\n",
    "        outliers_config['path_to_act_scales'], \n",
    "        outliers_config['fp_features_num']\n",
    "    )\n",
    "    model.replace_Linear(\n",
    "        outlier_ids=outlier_ids,\n",
    "        training_mode=config['QuantizedLinear']['training_mode'] \n",
    "    )\n",
    "\n",
    "if config['loading_quik_quant_weight']['load_weight']:\n",
    "    path_to_params = config['loading_quik_quant_weight']['path_to_quant_params']\n",
    "    learnable_scale = config['loading_quik_quant_weight']['learnable_scale']\n",
    "    quant_params = torch.load(path_to_params)\n",
    "\n",
    "    model.add_quant_weight(quant_params, learnable_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_ids = quant_params['model.layers.0.self_attn.q_proj']['fp_indices']\n",
    "fp_weight = quant_params['model.layers.0.self_attn.q_proj']['fp_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_weight = fp_weight[:, outlier_ids.sort()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = model.model.layers[0].self_attn.q_proj.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:, ~mask] = fp_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0349, -0.0059,  0.0396,  ..., -0.0160,  0.1276, -0.0387],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1., -2., -0.,  ...,  1.,  0., -1.],\n",
       "        [ 2., -0.,  0.,  ..., -1., -1.,  1.],\n",
       "        [-1.,  1.,  0.,  ...,  1.,  2., -0.],\n",
       "        ...,\n",
       "        [-0.,  1., -0.,  ...,  1., -2.,  1.],\n",
       "        [ 2.,  1.,  0.,  ..., -2., -1., -1.],\n",
       "        [-1., -1.,  0.,  ...,  2.,  2., -1.]], dtype=torch.float16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.model.layers[0].self_attn.q_proj.weight.data\n",
    "alpha = model.model.layers[0].self_attn.q_proj.quantizer.alpha_scale\n",
    "bit = model.model.layers[0].self_attn.q_proj.quantizer.bit\n",
    "qmax = 2**(bit-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0062, -0.0124, -0.0000,  ...,  0.0062,  0.0000, -0.0062],\n",
       "        [ 0.0209, -0.0000,  0.0000,  ..., -0.0104, -0.0104,  0.0104],\n",
       "        [-0.0103,  0.0103,  0.0000,  ...,  0.0103,  0.0207, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0129, -0.0000,  ...,  0.0129, -0.0258,  0.0129],\n",
       "        [ 0.0300,  0.0150,  0.0000,  ..., -0.0300, -0.0150, -0.0150],\n",
       "        [-0.0092, -0.0092,  0.0000,  ...,  0.0183,  0.0183, -0.0092]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha / qmax * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0349, -0.0059,  0.0396,  ..., -0.0160,  0.1276, -0.0387],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1., -2., -0.,  ...,  1.,  0., -1.],\n",
       "        [ 2., -0.,  0.,  ..., -1., -1.,  1.],\n",
       "        [-1.,  1.,  0.,  ...,  1.,  2., -0.],\n",
       "        ...,\n",
       "        [-0.,  1., -0.,  ...,  1., -2.,  1.],\n",
       "        [ 2.,  1.,  0.,  ..., -2., -1., -1.],\n",
       "        [-1., -1.,  0.,  ...,  2.,  2., -1.]], dtype=torch.bfloat16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/Quantization/weights_study/weights/llama7b_3bit_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/Quantization/weights_study/weights/llama7b_3bit_loaded/tokenizer_config.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/special_tokens_map.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/tokenizer.model',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/added_tokens.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/home/Quantization/weights_study/weights/llama7b_3bit_loaded\")\n",
    "tokenizer.save_pretrained(\"/home/Quantization/weights_study/weights/llama7b_3bit_loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/Quantization/weights_study/weights/llama7b_3bit_loaded/tokenizer_config.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/special_tokens_map.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/tokenizer.model',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/added_tokens.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_3bit_loaded/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/Quantization/weights_study/weights/llama7b_4bit_trained_scale/tokenizer_config.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_4bit_trained_scale/special_tokens_map.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_4bit_trained_scale/tokenizer.model',\n",
       " '/home/Quantization/weights_study/weights/llama7b_4bit_trained_scale/added_tokens.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_4bit_trained_scale/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/home/Quantization/weights_study/weights/llama7b_4bit_trained_scale\")\n",
    "tokenizer.save_pretrained(\"/home/Quantization/weights_study/weights/llama7b_4bit_trained_scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['loading_quik_quant_weight']['load_weight']:\n",
    "    path_to_params = config['loading_quik_quant_weight']['path_to_quant_params']\n",
    "    learnable_scale = config['loading_quik_quant_weight']['learnable_scale']\n",
    "    quant_params = torch.load(path_to_params)\n",
    "\n",
    "    model.add_quant_weight(quant_params, learnable_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_config = config['NoiseQuant']\n",
    "outliers_config = config['outliers']\n",
    "outlier_ids, layer_bit = prepare_llama_quant(\n",
    "    outliers_config['path_to_act_scales'], \n",
    "    outliers_config['fp_features_num'], \n",
    "    **noise_config['layer_bits']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp_inds_for_quik(path_to_act_scales, fp_features_num):\n",
    "    act_scales = torch.load(path_to_act_scales)\n",
    "    fp_indices_in_lin_layers = {k: torch.sort(v)[1][-fp_features_num:] for k, v in act_scales.items()}\n",
    "    return fp_indices_in_lin_layers\n",
    "\n",
    "noise_config = config['NoiseQuant']\n",
    "outliers_config = config['outliers']\n",
    "\n",
    "fp_inds_in_lin_layers = get_fp_inds_for_quik(outliers_config['path_to_act_scales'], outliers_config['fp_features_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.mlp.gate_proj\n",
      "model.layers.28.mlp.up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.mlp.gate_proj\n",
      "model.layers.29.mlp.up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.mlp.gate_proj\n",
      "model.layers.30.mlp.up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.mlp.gate_proj\n",
      "model.layers.31.mlp.up_proj\n",
      "model.layers.31.mlp.down_proj\n"
     ]
    }
   ],
   "source": [
    "outlier_fraction = 0.05 \n",
    "\n",
    "modules_name_dict = {name: module for name, module in model.named_modules()}\n",
    "for name, module in modules_name_dict.items():\n",
    "    if isinstance(module, nn.Linear) and (name.find('lm_head') == -1):\n",
    "        ind = name.rfind(\".\")\n",
    "        if ind == -1:\n",
    "            father = modules_name_dict[\"\"]\n",
    "        else:\n",
    "            father = modules_name_dict[name[:ind]]\n",
    "        print(name)\n",
    "        fp_indices = fp_inds_in_lin_layers[name]\n",
    "\n",
    "        weight = module.weight.data\n",
    "        mask = torch.ones(weight.size(1), dtype=torch.bool)\n",
    "        mask[fp_indices] = False\n",
    "\n",
    " \n",
    "        with torch.no_grad(): \n",
    "            w = weight[:, mask] \n",
    "            w_flat = w.view(-1) \n",
    "            lower_threshold, upper_threshold = ( \n",
    "                torch.kthvalue( \n",
    "                    w_flat, \n",
    "                    int(w_flat.numel() * outlier_fraction / 2), \n",
    "                )[0], \n",
    "                torch.kthvalue( \n",
    "                    w_flat, \n",
    "                    int(w_flat.numel() * (1 - outlier_fraction / 2)), \n",
    "                )[0], \n",
    "            ) \n",
    "        \n",
    "            outliers = (w < lower_threshold) | (w > upper_threshold) \n",
    "        \n",
    "            outlier_mask = outliers.detach()\n",
    "            w[outlier_mask] = 0\n",
    "\n",
    "            module.weight.data[:, mask] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/Quantization/weights_study/weights/llama7b_no_outliers_in_quant_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/Quantization/weights_study/weights/llama7b_no_outliers_in_quant_weight/tokenizer_config.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_no_outliers_in_quant_weight/special_tokens_map.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_no_outliers_in_quant_weight/tokenizer.model',\n",
       " '/home/Quantization/weights_study/weights/llama7b_no_outliers_in_quant_weight/added_tokens.json',\n",
       " '/home/Quantization/weights_study/weights/llama7b_no_outliers_in_quant_weight/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/home/Quantization/weights_study/weights/llama7b_no_outliers_in_quant_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2715, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight[:, mask].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_quant = weight[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0203, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.weight.data[:, mask].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quik_dict = torch.load(\"/home/LLM_compression/QUIK/weights/llama7b_3bit_128fp_quant_scales/quant_params.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_indices = quik_dict['model.layers.25.mlp.up_proj']['fp_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.col_perm = act_scales.sort()[1]\n",
    "self.inv_col_perm = torch.zeros_like(self.col_perm)\n",
    "self.inv_col_perm[self.col_perm] = torch.arange(self.col_perm.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  16,   23,   93,   94,  149,  257,  264,  282,  310,  339,  363,  386,\n",
       "         420,  436,  448,  462,  468,  470,  488,  490,  588,  597,  641,  788,\n",
       "         888,  934,  972, 1214, 1215, 1331, 1335, 1345, 1360, 1363, 1379, 1404,\n",
       "        1415, 1432, 1465, 1494, 1512, 1571, 1605, 1619, 1678, 1688, 1755, 1763,\n",
       "        1793, 1825, 1839, 1845, 1916, 1946, 2010, 2016, 2033, 2036, 2050, 2056,\n",
       "        2084, 2094, 2158, 2168, 2192, 2209, 2215, 2230, 2260, 2281, 2298, 2324,\n",
       "        2350, 2358, 2360, 2369, 2389, 2393, 2459, 2465, 2469, 2533, 2573, 2611,\n",
       "        2622, 2647, 2750, 2789, 2852, 2853, 2863, 2883, 2916, 2924, 2927, 2980,\n",
       "        3002, 3038, 3061, 3173, 3178, 3202, 3208, 3215, 3241, 3391, 3444, 3471,\n",
       "        3546, 3571, 3651, 3656, 3700, 3766, 3803, 3826, 3839, 3844, 3872, 3952,\n",
       "        3971, 3997, 4030, 4051, 4053, 4071, 4074, 4076])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_indices.sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quik_dict['model.layers.25.mlp.up_proj']['quant_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = quik_dict['model.layers.25.mlp.up_proj']['quant_weight']\n",
    "alpha_scale = quik_dict['model.layers.25.mlp.up_proj']['alpha'].to('cpu')\n",
    "qmax = quik_dict['model.layers.25.mlp.up_proj']['maxq'].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = alpha_scale / qmax * weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3., -1.,  2.,  ...,  1.,  1., -2.],\n",
       "        [-2.,  0.,  1.,  ..., -1.,  1., -0.],\n",
       "        [ 2., -0.,  2.,  ..., -1., -1., -0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  3.,  ...,  1., -3., -2.],\n",
       "        [-2.,  1., -3.,  ..., -3., -2.,  0.],\n",
       "        [-1.,  0.,  2.,  ...,  2.,  1.,  2.]], dtype=torch.float16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mweight\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weight' is not defined"
     ]
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = quik_dict['model.layers.25.mlp.up_proj']['quant_weight'].data.clone()\n",
    "mask = torch.ones(weight.size(1), dtype=torch.bool)\n",
    "mask[fp_indices] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ids = torch.arange(weight.size(1))\n",
    "col_perm = torch.cat([col_ids[mask], col_ids[~mask]])\n",
    "inv_col_perm = torch.zeros(col_perm.numel(), dtype=col_perm.dtype)\n",
    "inv_col_perm[col_perm] = torch.arange(col_perm.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = weight[:, col_perm].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = weight[:, inv_col_perm].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weight != quik_dict['model.layers.25.mlp.up_proj']['quant_weight'].data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 3965, 3966, 3967])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_col_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_perm = fp_indices.sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4096)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [4096] cannot be broadcast to indexing result of shape [128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m quik_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.layers.25.mlp.up_proj\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquant_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m inv_col_perm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(num_cols)\n\u001b[0;32m----> 4\u001b[0m inv_col_perm[col_perm] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(num_cols)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [4096] cannot be broadcast to indexing result of shape [128]"
     ]
    }
   ],
   "source": [
    "num_cols = .shape[1]\n",
    "\n",
    "inv_col_perm = torch.zeros(num_cols)\n",
    "inv_col_perm[col_perm] = torch.arange(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  16,   23,   93,   94,  149,  257,  264,  282,  310,  339,  363,  386,\n",
       "         420,  436,  448,  462,  468,  470,  488,  490,  588,  597,  641,  788,\n",
       "         888,  934,  972, 1214, 1215, 1331, 1335, 1345, 1360, 1363, 1379, 1404,\n",
       "        1415, 1432, 1465, 1494, 1512, 1571, 1605, 1619, 1678, 1688, 1755, 1763,\n",
       "        1793, 1825, 1839, 1845, 1916, 1946, 2010, 2016, 2033, 2036, 2050, 2056,\n",
       "        2084, 2094, 2158, 2168, 2192, 2209, 2215, 2230, 2260, 2281, 2298, 2324,\n",
       "        2350, 2358, 2360, 2369, 2389, 2393, 2459, 2465, 2469, 2533, 2573, 2611,\n",
       "        2622, 2647, 2750, 2789, 2852, 2853, 2863, 2883, 2916, 2924, 2927, 2980,\n",
       "        3002, 3038, 3061, 3173, 3178, 3202, 3208, 3215, 3241, 3391, 3444, 3471,\n",
       "        3546, 3571, 3651, 3656, 3700, 3766, 3803, 3826, 3839, 3844, 3872, 3952,\n",
       "        3971, 3997, 4030, 4051, 4053, 4071, 4074, 4076])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 4093, 4094, 4095])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_col_perm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 4093, 4094, 4095])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_indices = quik_dict['model.layers.25.mlp.up_proj']['fp_indices']\n",
    "fp_weight = quik_dict['model.layers.25.mlp.up_proj']['fp_weight']\n",
    "bit = quik_dict['model.layers.25.mlp.up_proj']['bit']\n",
    "alpha = quik_dict['model.layers.25.mlp.up_proj']['alpha']\n",
    "maxq = 2 ** (bit -1) - 1 \n",
    "scale = alpha / maxq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_name_dict = {name: module for name, module in model.named_modules()}\n",
    "        for name, module in modules_name_dict.items():\n",
    "            if isinstance(module, nn.Linear) and (name.find('lm_head') == -1):\n",
    "                ind = name.rfind(\".\")\n",
    "                if ind == -1:\n",
    "                    father = modules_name_dict[\"\"]\n",
    "                else:\n",
    "                    father = modules_name_dict[name[:ind]]\n",
    "                print(name)\n",
    "                fp_cols_inds = fp_inds_in_lin_layers[name]\n",
    "                qlinear = LinearQuantNoise(\n",
    "                    module.weight, module.bias, \n",
    "                    quant_bit=config['LinearQuantNoise']['quant_bit'], \n",
    "                    block_size=config['LinearQuantNoise']['block_size'], \n",
    "                    fp_cols_inds=fp_cols_inds, \n",
    "                    training_mode=config['LinearQuantNoise']['training_mode'], \n",
    "                    add_quant_noise=config['LinearQuantNoise']['add_quant_noise']\n",
    "                )\n",
    "                qlinear.get_quant_scales()\n",
    "                setattr(father, name[ind + 1:], qlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/LLM_compression/QUIK/weights/llama7b_2w_16a_128fp_quant_with_trained_scales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/LLM_compression/QUIK/weights/llama7b_2w_16a_128fp_quant_with_trained_scales/tokenizer_config.json',\n",
       " '/home/LLM_compression/QUIK/weights/llama7b_2w_16a_128fp_quant_with_trained_scales/special_tokens_map.json',\n",
       " '/home/LLM_compression/QUIK/weights/llama7b_2w_16a_128fp_quant_with_trained_scales/tokenizer.model',\n",
       " '/home/LLM_compression/QUIK/weights/llama7b_2w_16a_128fp_quant_with_trained_scales/added_tokens.json',\n",
       " '/home/LLM_compression/QUIK/weights/llama7b_2w_16a_128fp_quant_with_trained_scales/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/home/LLM_compression/QUIK/weights/llama7b_2w_16a_128fp_quant_with_trained_scales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0039,  0.0415,  ..., -0.0312,  0.1289, -0.0405],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0117, -0.0233,  0.0000,  ...,  0.0000,  0.0000, -0.0117],\n",
       "        [ 0.0156, -0.0156,  0.0000,  ..., -0.0156, -0.0156,  0.0000],\n",
       "        [-0.0203,  0.0101,  0.0000,  ...,  0.0101,  0.0101, -0.0101],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0542,  0.0000],\n",
       "        [ 0.0216,  0.0000,  0.0000,  ..., -0.0432, -0.0216, -0.0216],\n",
       "        [-0.0356, -0.0178,  0.0000,  ...,  0.0178,  0.0178, -0.0178]],\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0039,  0.0415,  ..., -0.0312,  0.1289, -0.0405],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0039,  0.0415,  ..., -0.0312,  0.1289, -0.0405],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0092,  0.0415,  ..., -0.0162,  0.1289, -0.0405],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.quantizer.alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'quantizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     cur_projection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(cur_layer, proj_name)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# if isinstance(cur_projection, QuantizedLinear):\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     layer_weight_num \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mcur_projection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241m.\u001b[39mweight_shape)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'quantizer'"
     ]
    }
   ],
   "source": [
    "#Optimization\n",
    "# from transformers.modeling_utils import unwrap_model\n",
    "\n",
    "model_name  = model._get_name()\n",
    "if model_name in ['LlamaForCausalLM']:\n",
    "    pass\n",
    "\n",
    "decoder_layer = model.model.layers[0]\n",
    "\n",
    "\n",
    "layers = ['self_attn', 'mlp']\n",
    "projectors = {\n",
    "    'self_attn': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    'mlp': ['up_proj', 'down_proj', 'gate_proj']\n",
    "}\n",
    "for layer_name in layers:\n",
    "    # cur_layer = getattr(self, layer_name)\n",
    "    cur_layer = getattr(decoder_layer, layer_name)\n",
    "    for proj_name in projectors[layer_name]:\n",
    "        cur_projection = getattr(cur_layer, proj_name)\n",
    "\n",
    "        # if isinstance(cur_projection, QuantizedLinear):\n",
    "        layer_weight_num = torch.tensor(cur_projection.quantizer.weight_shape).sum()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsq_forward(w, bit, alpha):    \n",
    "    qmax = 2 ** (bit.detach() - 1) - 1\n",
    "    # q = F.hardtanh(w / alpha, -1.0, 1.0) * qmax\n",
    "    q = F.hardtanh(w / alpha, -1.0, 1.0)\n",
    "    mask_q_pos = (q > 0)\n",
    "    q = q * (2 ** (bit.detach() - 1)) - mask_q_pos * q\n",
    "    out = (q.round() + (q - q.detach())) * (alpha / qmax)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoiseQuant' object has no attribute 'quant_cols_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcur_projection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_cols_num\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoiseQuant' object has no attribute 'quant_cols_num'"
     ]
    }
   ],
   "source": [
    "cur_projection.quantizer.quant_cols_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit = cur_projection.quantizer.bit\n",
    "bit = torch.tensor(bit)\n",
    "block_size = cur_projection.quantizer.block_size\n",
    "mask = cur_projection.quantizer.mask\n",
    "quant_cols_num = 31\n",
    "w = cur_projection.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask is not None:\n",
    "    w_re = w[:, mask]\n",
    "else:\n",
    "    w_re = w\n",
    "\n",
    "if block_size > 0:\n",
    "    out_features = w_re.shape[0]\n",
    "    in_features = w_re.shape[1]\n",
    "    # w_re = w_re.reshape((out_features * block_size, in_features // block_size))\n",
    "       \n",
    "    w_re = w_re.reshape((out_features * quant_cols_num, block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha0 = 0.01*torch.ones(out_features, in_features // block_size, dtype=w.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha0 = nn.Parameter(0.01*torch.ones(out_features, in_features // block_size, dtype=w.dtype))\n",
    "alpha = nn.Parameter(0.01*torch.ones(out_features * in_features // block_size, 1, dtype=w.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = torch.repeat_interleave(alpha0, block_size, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit = nn.Parameter(torch.zeros(1))\n",
    "# alpha = nn.Parameter(torch.tensor(0.01))\n",
    "\n",
    "N_BIN = 256\n",
    "# bit = 2 + torch.sigmoid(bit)*4\n",
    "bit = 1.5 + torch.sigmoid(bit)\n",
    "\n",
    "bit += (torch.rand_like(bit) - 0.5)\n",
    "bit = bit.round() + (bit - bit.detach())\n",
    "\n",
    "alpha = F.softplus(alpha, beta=10**(6), threshold=1) \n",
    "lsq = lsq_forward(w_re, bit.round(), alpha)\n",
    "\n",
    "c1 = w_re >= alpha\n",
    "c2 = w_re <= -alpha     \n",
    "delta = alpha / (2**(bit - 1) - 1)\n",
    "\n",
    "with torch.no_grad():                \n",
    "    diff = (lsq - w_re) / delta #difference between dequantized and original weights after their scale\n",
    "    sel = diff[torch.logical_not(torch.logical_or(c1, c2))] #take weights less than alpha\n",
    "\n",
    "    hist = torch.histc(sel, bins=N_BIN, min=-0.5, max=0.5)    \n",
    "\n",
    "    noise = torch.multinomial(hist, w_re.numel(), True) + torch.rand_like(w_re.view(-1))               \n",
    "    noise = (noise / N_BIN - 0.5).view(w_re.shape)\n",
    "    noise = noise.to(w_re.dtype)\n",
    "\n",
    "w_rand = noise * delta\n",
    "w_cliped = torch.where(c2, -alpha, w_re + w_rand)\n",
    "w_cliped = torch.where(c1, alpha, w_cliped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask is not None:\n",
    "    w_out = torch.zeros(w.shape, dtype=w.dtype, device=w.device)\n",
    "    w_out[:, mask] = w_cliped.reshape((out_features, in_features))\n",
    "    w_out[:, ~mask] = w[:, ~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0087,  0.0415,  ..., -0.0162,  0.1289, -0.0405],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_out[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.ones(3, 3)\n",
    "print(x1)\n",
    "pad_value = 0\n",
    "pad_func = nn.ConstantPad1d((0, 1), pad_value)\n",
    "\n",
    "output_t = pad_func(x1)\n",
    "output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([524288, 31])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_cliped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rand = w_rand.reshape((out_features, in_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rand_ext = torch.zeros(w.shape, dtype=w.dtype, device=w.device)\n",
    "w_rand_ext[:, mask] = w_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (31) must match the size of tensor b (4096) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w_cliped1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_rand_ext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (31) must match the size of tensor b (4096) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "w_cliped1 = torch.where(c2, 0, w )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (31) must match the size of tensor b (3968) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mw_re\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_rand\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (31) must match the size of tensor b (3968) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "w_re + w_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (31) must match the size of tensor b (3968) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mwhere(c2, \u001b[38;5;241m-\u001b[39malpha, \u001b[43mw_re\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_rand\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (31) must match the size of tensor b (3968) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.where(c2, -alpha, w_re + w_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([524288, 31])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4096) must match the size of tensor b (3968) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_rand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4096) must match the size of tensor b (3968) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.where(mask, w_rand, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0004, -0.0036,  0.0023,  ...,  0.0023, -0.0033,  0.0024],\n",
       "        [ 0.0012,  0.0018, -0.0005,  ...,  0.0031, -0.0007,  0.0005],\n",
       "        [ 0.0005,  0.0020, -0.0011,  ...,  0.0045, -0.0020, -0.0036],\n",
       "        ...,\n",
       "        [ 0.0005, -0.0047,  0.0018,  ...,  0.0013,  0.0009, -0.0010],\n",
       "        [ 0.0024,  0.0008, -0.0007,  ..., -0.0048,  0.0020,  0.0005],\n",
       "        [-0.0018, -0.0005, -0.0002,  ...,  0.0013, -0.0023, -0.0033]],\n",
       "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_rand_ext.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8775)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.5 + torch.sigmoid(torch.tensor(-0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7293e-03, -1.0000e-02, -2.6828e-03,  ...,  4.3774e-03,\n",
       "          2.2817e-03, -3.7865e-03],\n",
       "        [ 1.3864e-02, -3.9429e-03,  3.4691e-03,  ..., -9.9187e-03,\n",
       "         -1.0000e-02,  7.2362e-03],\n",
       "        [-1.0000e-02,  1.2224e-02,  2.3237e-05,  ...,  5.5835e-03,\n",
       "          1.8989e-02, -2.9551e-03],\n",
       "        ...,\n",
       "        [ 1.1344e-03,  1.0702e-02,  2.2955e-04,  ...,  1.0329e-02,\n",
       "         -1.0000e-02,  1.0352e-02],\n",
       "        [ 2.5054e-02,  1.0310e-02,  3.3550e-03,  ..., -1.0000e-02,\n",
       "         -1.0000e-02, -1.0000e-02],\n",
       "        [-1.0000e-02, -6.1651e-03,  1.6134e-03,  ...,  1.8652e-02,\n",
       "          1.6144e-02, -8.9261e-03]], grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(c2, -alpha, w + noise * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(c1, alpha, torch.where(c2, -alpha, data + noise * delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9492, 0.5703, 0.5078,  ..., 0.0938, 0.8867, 0.8672],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand_like(w.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([240.0000, 170.0000,   7.1875,  ...,  91.0000, 108.0000, 165.0000],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(hist, w.numel(), True) + torch.rand_like(w.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2617, -0.0547, -0.1953,  ..., -0.1270, -0.4590, -0.3906],\n",
       "        [-0.4922,  0.2305, -0.0566,  ...,  0.4180, -0.1035,  0.2266],\n",
       "        [ 0.2891, -0.4648, -0.2461,  ..., -0.2363,  0.0898, -0.4512],\n",
       "        ...,\n",
       "        [-0.4375, -0.2637,  0.0781,  ..., -0.4062, -0.3359, -0.1836],\n",
       "        [-0.1797, -0.3203,  0.1484,  ..., -0.1094,  0.2070,  0.1836],\n",
       "        [ 0.1133, -0.3320,  0.2383,  ...,  0.2578, -0.2090, -0.4805]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = True\n",
    "is_discretize = True\n",
    "if not is_training or is_discretize :\n",
    "    bit = bit.round() + (bit - bit.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3133], grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = F.softplus(alpha)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=4096, bias=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.q_proj.quantizer.quant_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['QuantizedLinear']['replace']:\n",
    "    outliers_config= config['outliers']\n",
    "    outlier_ids, layer_bit = prepare_llama_quant(\n",
    "        outliers_config['path_to_act_scales'], \n",
    "        outliers_config['fp_features_num']\n",
    "    )\n",
    "\n",
    "    model.replace_Linear(\n",
    "        outlier_ids=outlier_ids,\n",
    "        training_mode=config['QuantizedLinear']['training_mode'] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): QuantizedLinear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): QuantizedLinear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): QuantizedLinear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): QuantizedLinear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantizedLinear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): QuantizedLinear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): QuantizedLinear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = model.model.layers[0].self_attn.q_proj.weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       dtype=torch.bfloat16, grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_orig = model.model.layers[0].self_attn.q_proj.weight.clone()\n",
    "w_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0085,  0.0415,  ..., -0.0162,  0.1289, -0.0405],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_train[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0087,  0.0415,  ..., -0.0162,  0.1289, -0.0405],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_orig[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0110, -0.0220,  0.0000,  ...,  0.0000,  0.0000, -0.0089],\n",
       "        [ 0.0165, -0.0165,  0.0000,  ..., -0.0153, -0.0153,  0.0000],\n",
       "        [-0.0259,  0.0129,  0.0000,  ...,  0.0121,  0.0121, -0.0121],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0206,  0.0000,  ...,  0.0000, -0.0510,  0.0000],\n",
       "        [ 0.0236,  0.0000,  0.0000,  ..., -0.0369, -0.0369, -0.0184],\n",
       "        [-0.0300, -0.0150,  0.0000,  ...,  0.0168,  0.0168, -0.0168]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0110],\n",
       "        [0.0096],\n",
       "        [0.0069],\n",
       "        ...,\n",
       "        [0.0167],\n",
       "        [0.0138],\n",
       "        [0.0168]], device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_train = model.model.layers[0].self_attn.q_proj.quantizer.alpha\n",
    "alpha_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0110],\n",
       "        [0.0096],\n",
       "        [0.0069],\n",
       "        ...,\n",
       "        [0.0167],\n",
       "        [0.0138],\n",
       "        [0.0168]], dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_train = model.model.layers[0].self_attn.q_proj.quantizer.alpha\n",
    "alpha_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0109],\n",
       "        [0.0095],\n",
       "        [0.0070],\n",
       "        ...,\n",
       "        [0.0167],\n",
       "        [0.0137],\n",
       "        [0.0167]], device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_orig = model.model.layers[0].self_attn.q_proj.quantizer.alpha\n",
    "alpha_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after train\n",
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0087,  0.0415,  ..., -0.0162,  0.1289, -0.0405],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0087,  0.0415,  ..., -0.0164,  0.1289, -0.0405],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after train\n",
    "model.model.layers[0].self_attn.q_proj.weight[:, 3190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['NoiseQuant']['add_quant_noise']:\n",
    "    noise_config = config['NoiseQuant']\n",
    "    outliers_config= config['outliers']\n",
    "    outlier_ids, layer_bit = prepare_llama_quant(\n",
    "        outliers_config['path_to_act_scales'], \n",
    "        outliers_config['fp_features_num'], \n",
    "        **noise_config['layer_bits']\n",
    "    )\n",
    "    model.add_quant_noise_to_weight( \n",
    "        layer_bit=layer_bit, \n",
    "        block_size=noise_config['block_size'],\n",
    "        fp_cols_num=outliers_config['fp_features_num'],\n",
    "        compute_scale=noise_config['compute_scale'], \n",
    "        quant_noise_predict=noise_config['predict']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['BitNoiseQuant']['add_quant_noise']:\n",
    "    noise_config = config['BitNoiseQuant']\n",
    "    outliers_config= config['outliers']\n",
    "    outlier_ids, layer_bit = prepare_llama_quant(\n",
    "        outliers_config['path_to_act_scales'], \n",
    "        outliers_config['fp_features_num'], \n",
    "        **noise_config['layer_bits']\n",
    "    )\n",
    "    model.add_quant_bitnoise_to_weight( \n",
    "        layer_bit=layer_bit, \n",
    "        block_size=noise_config['block_size'],\n",
    "        fp_cols_num=outliers_config['fp_features_num'],\n",
    "        compute_scale=noise_config['compute_scale'], \n",
    "        quant_noise_predict=noise_config['predict']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.quantizer.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/Quantization/weights_study/weights/llama-2-7b-wrand-2bit-each-iter/tokenizer_config.json',\n",
       " '/home/Quantization/weights_study/weights/llama-2-7b-wrand-2bit-each-iter/special_tokens_map.json',\n",
       " '/home/Quantization/weights_study/weights/llama-2-7b-wrand-2bit-each-iter/tokenizer.model',\n",
       " '/home/Quantization/weights_study/weights/llama-2-7b-wrand-2bit-each-iter/added_tokens.json',\n",
       " '/home/Quantization/weights_study/weights/llama-2-7b-wrand-2bit-each-iter/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('/home/Quantization/weights_study/weights/llama-2-7b-wrand-2bit-each-iter')\n",
    "tokenizer.save_pretrained('/home/Quantization/weights_study/weights/llama-2-7b-wrand-2bit-each-iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0354, -0.0085,  0.0415,  ..., -0.0162,  0.1289, -0.0405],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0042, -0.0109, -0.0019,  ...,  0.0014,  0.0023, -0.0038],\n",
       "        [ 0.0149, -0.0063, -0.0031,  ..., -0.0135, -0.0069,  0.0061],\n",
       "        [-0.0130,  0.0150,  0.0035,  ...,  0.0023,  0.0123, -0.0027],\n",
       "        ...,\n",
       "        [ 0.0064,  0.0075,  0.0090,  ...,  0.0037, -0.0255,  0.0149],\n",
       "        [ 0.0237,  0.0002,  0.0055,  ..., -0.0184, -0.0078, -0.0099],\n",
       "        [-0.0208, -0.0118,  0.0067,  ...,  0.0167,  0.0193, -0.0113]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.quantizer.quant_noise(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'quant_scale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_scale\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'quant_scale'"
     ]
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.quantizer.quant_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): QuantizedLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (quantizer): BitNoiseQuant()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0109],\n",
       "        [0.0095],\n",
       "        [0.0070],\n",
       "        ...,\n",
       "        [0.0167],\n",
       "        [0.0137],\n",
       "        [0.0167]], device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.quantizer.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config['change_training_mode']:\n",
    "#     outliers_config= config['outliers']\n",
    "#     outlier_ids, _ = prepare_llama_quant(\n",
    "#         outliers_config['path_to_act_scales'], \n",
    "#         outliers_config['fp_features_num']\n",
    "#     )\n",
    "#     training_mode = config['change_training_mode']\n",
    "#     model.change_training_mode(outlier_ids, training_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32001 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing and reformatting instruction data (num_proc=8): 100%|██████████| 30984/30984 [00:20<00:00, 1515.09 examples/s]\n",
      "Tokenizing and reformatting instruction data (num_proc=8): 100%|██████████| 1631/1631 [00:01<00:00, 1217.31 examples/s]\n",
      "Filter: 100%|██████████| 30984/30984 [00:02<00:00, 10556.06 examples/s]\n",
      "Filter: 100%|██████████| 1631/1631 [00:00<00:00, 10992.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Load and preprocessing dataset\n",
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map\n",
    "if isinstance(tokenizer, LlamaTokenizer) or isinstance(tokenizer, LlamaTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens in [0, 1], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(len(tokenizer), embedding_size)\n",
    "\n",
    "raw_datasets = load_hf_datasets(data_args)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_prompt_completion_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "\n",
    "lm_datasets = raw_datasets.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    remove_columns=[name for name in raw_datasets[\"train\"].column_names if name not in [\"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "    desc=\"Tokenizing and reformatting instruction data\",\n",
    ")\n",
    "\n",
    "lm_datasets.set_format(type=\"pt\")\n",
    "lm_datasets = lm_datasets.filter(lambda example: (example['labels'] != -100).any())\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones((1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :, torch.tensor([False, True, True])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.q_proj.quantizer.alpha\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    name = name.replace('.weight', '')\n",
    "    if name.find('model.layers.0.self_attn.q_proj') != -1:\n",
    "    # if name.find('model.layers.0.mlp.up_proj') != -1:\n",
    "        print(name)\n",
    "        param.requires_grad_()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lm_head'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_params: 16904192\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(f\"trainable_params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=\"longest\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhelninmax\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/LLM_compression/llm-tune/wandb/run-20240225_115952-skkzyct6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhelninmax/huggingface/runs/skkzyct6' target=\"_blank\">copper-tree-110</a></strong> to <a href='https://wandb.ai/zhelninmax/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhelninmax/huggingface' target=\"_blank\">https://wandb.ai/zhelninmax/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhelninmax/huggingface/runs/skkzyct6' target=\"_blank\">https://wandb.ai/zhelninmax/huggingface/runs/skkzyct6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 08:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.961600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /home/exp_results/output/instruct/llama7b_test_noise/checkpoint-1 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /home/exp_results/output/instruct/llama7b_test_noise/checkpoint-2 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
