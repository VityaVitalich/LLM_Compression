{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ()['HF_HOME'] = '/home/data/taxonomy/hf_cache/'\n",
    "# os.environ()['TRANSFORMERS_CACHE'] = '/home/data/taxonomy/hf_cache/'\n",
    "\n",
    "\n",
    "from rpca import R_pca\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.rand(30,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.matrix_rank(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpca = R_pca(M.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, error: 0.27869155103304716\n",
      "iteration: 100, error: 0.0027078217244207643\n",
      "iteration: 200, error: 0.0002764315094933198\n",
      "iteration: 300, error: 9.25628969698951e-05\n",
      "iteration: 400, error: 4.909344810913709e-05\n",
      "iteration: 500, error: 2.4804079049332576e-05\n",
      "iteration: 600, error: 1.3823971474745666e-05\n",
      "iteration: 700, error: 7.902946100797921e-06\n",
      "iteration: 800, error: 4.517527693640376e-06\n",
      "iteration: 900, error: 2.5776385862268554e-06\n",
      "iteration: 1000, error: 1.4953778853601025e-06\n",
      "iteration: 1100, error: 8.76278024641721e-07\n",
      "iteration: 1200, error: 5.154393265697054e-07\n",
      "iteration: 1300, error: 3.0801674507270573e-07\n",
      "iteration: 1400, error: 1.8544578478649308e-07\n",
      "iteration: 1500, error: 1.123558048334627e-07\n",
      "iteration: 1600, error: 6.846994873692011e-08\n",
      "iteration: 1700, error: 4.196292105398493e-08\n",
      "iteration: 1800, error: 2.586460062965698e-08\n",
      "iteration: 1900, error: 1.603521792915417e-08\n",
      "iteration: 2000, error: 1.0001054985939126e-08\n",
      "iteration: 2001, error: 9.954258651409416e-09\n"
     ]
    }
   ],
   "source": [
    "L, S = rpca.fit(max_iter=100000, iter_print=100, tol=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.matrix_rank(torch.tensor(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(S, 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = np.linalg.svd(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 30), (40, 40), (30,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, V.shape, S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 17\n",
    "new = U[:,:r]@np.diag(S[:r])@V[:r,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (30,40) (30,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnew\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (30,40) (30,) "
     ]
    }
   ],
   "source": [
    "new + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~np.isclose(new, L)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_and_back(M, rank):\n",
    "    rpca = R_pca(M.float().numpy())\n",
    "    L, S = rpca.fit(max_iter=10, iter_print=1, tol=1e-7)\n",
    "\n",
    "    U, Sigma, V = np.linalg.svd(L)\n",
    "    new = U[:,:rank]@np.diag(Sigma[:rank])@V[:rank,:]\n",
    "\n",
    "    return torch.tensor(new + S, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/LLM_Compression/transformers_modified/src/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f49bb375f4e49a9afe41d4dd474709e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = 'meta-llama/Llama-2-7b-hf'\n",
    "token = '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'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        use_auth_token=token, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        cache_dir='/home/data/taxonomy/hf_cache/'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.0.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.0.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.0.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.0.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.0.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.0.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.1.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.1.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.1.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.1.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.1.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.1.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.1.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.2.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.2.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.2.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.2.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.2.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.2.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.2.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.3.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.3.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.3.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.3.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.3.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.3.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.3.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.4.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.4.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.4.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.4.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.4.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.4.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.4.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.5.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.5.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.5.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.5.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.5.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.5.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.5.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.6.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.6.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.6.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.6.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.6.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.6.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.6.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.7.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.7.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.7.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.7.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.7.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.7.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.7.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.8.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.8.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.8.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.8.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.8.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.8.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.8.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.9.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.9.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.9.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.9.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.9.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.9.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.9.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.10.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.10.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.10.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.10.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.10.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.10.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.10.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.11.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.11.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.11.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.11.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.11.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.11.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.11.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.12.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.12.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.12.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.12.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.12.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.12.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.12.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.13.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.13.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.13.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.13.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.13.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.13.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.13.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.14.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.14.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.14.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.14.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.14.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.14.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.14.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.15.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.15.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.15.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.15.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.15.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.15.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.15.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.16.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.16.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.16.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.16.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.16.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.16.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.16.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.17.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.17.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.17.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.17.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.17.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.17.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.17.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.18.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.18.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.18.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.18.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.18.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.18.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.18.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.19.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.19.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.19.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.19.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.19.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.19.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.19.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.20.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.20.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.20.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.20.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.20.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.20.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.20.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.21.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.21.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.21.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.21.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.21.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.21.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.21.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.22.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.22.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.22.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.22.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.22.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.22.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.22.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.23.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.23.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.23.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.23.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.23.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.23.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.23.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.24.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.24.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.24.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.24.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.24.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.24.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.24.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.25.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.25.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.25.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.25.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.25.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.25.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.25.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.26.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.26.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.26.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.26.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.26.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.26.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.26.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.27.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.27.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.27.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.27.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.27.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.27.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.27.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.28.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.28.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.28.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.28.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.28.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.28.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.28.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.29.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.29.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.29.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.29.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.29.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.29.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.29.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.30.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.30.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.30.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.30.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.30.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.30.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.30.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.31.self_attn.q_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.31.self_attn.k_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.31.self_attn.v_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.31.self_attn.o_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.31.mlp.gate_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.31.mlp.up_proj.weight <class 'torch.nn.parameter.Parameter'>\n",
      "model.layers.31.mlp.down_proj.weight <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if ('layers' in name) and (('mlp' in name) or ('self_attn' in name)):\n",
    "        print(name, type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, error: 0.2759443280539783\n",
      "iteration: 2, error: 0.0043662812881272274\n",
      "iteration: 3, error: 0.0025386843190319942\n",
      "iteration: 4, error: 0.0024512282882472843\n",
      "iteration: 5, error: 0.0024890273592615203\n",
      "iteration: 6, error: 0.002547910092984881\n",
      "iteration: 7, error: 0.0025943125417766276\n",
      "iteration: 8, error: 0.0026815332958747156\n",
      "iteration: 9, error: 0.0027602242751970196\n",
      "iteration: 10, error: 0.0028544054441271066\n"
     ]
    }
   ],
   "source": [
    "tens = torch.rand(30,40, dtype=torch.bfloat16)\n",
    "new_param = decompose_and_back(tens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2852, 0.7812, 0.2832,  ..., 0.2832, 0.8281, 0.6367],\n",
       "        [0.7930, 0.1406, 0.7773,  ..., 0.7617, 0.1846, 0.6602],\n",
       "        [0.1172, 0.2246, 0.3438,  ..., 0.2676, 0.8281, 0.7266],\n",
       "        ...,\n",
       "        [0.2715, 0.3984, 0.6875,  ..., 0.0684, 0.8633, 0.7031],\n",
       "        [0.5742, 0.1904, 0.6406,  ..., 0.7266, 0.4512, 0.6719],\n",
       "        [0.4531, 0.4609, 0.5781,  ..., 0.7070, 0.3770, 0.4863]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
